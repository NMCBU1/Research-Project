{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentiment_analysis_spanish import sentiment_analysis\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"consolidado_cantidad_casos_criminalidad_por_anio_mes.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(date1, date2):\n",
    "    for n in range(int ((date2 - date1).days)+1):\n",
    "        yield date1 + datetime.timedelta(n)\n",
    "\n",
    "start_dt = datetime.date(2003, 1, 1)\n",
    "end_dt = datetime.date(2008, 12, 1)\n",
    "for dt in daterange(start_dt, end_dt):\n",
    "    df.drop(df[dt.strftime(\"%Y-%m\") == df[\"Fecha_hecho\"]].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = []\n",
    "dates = []\n",
    "for date in df['Fecha_hecho'].unique():\n",
    "    df2 = pd.read_csv(date + \"_clean.csv\", encoding = \"latin\")\n",
    "    nums.append(df2[\"tweet\"].count())\n",
    "    dates.append(date)\n",
    "df3 = pd.DataFrame({\"dates\": dates,  \"count\": nums})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424.2875816993464"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3[\"count\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha_hecho</th>\n",
       "      <th>Conducta</th>\n",
       "      <th>Cantidad_casos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>2009-01</td>\n",
       "      <td>Homicidio</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>2009-02</td>\n",
       "      <td>Homicidio</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>2009-03</td>\n",
       "      <td>Homicidio</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2009-04</td>\n",
       "      <td>Homicidio</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>2009-05</td>\n",
       "      <td>Homicidio</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>2021-05</td>\n",
       "      <td>Homicidio</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>2021-06</td>\n",
       "      <td>Homicidio</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2021-07</td>\n",
       "      <td>Homicidio</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>2021-08</td>\n",
       "      <td>Homicidio</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>2021-09</td>\n",
       "      <td>Homicidio</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Fecha_hecho   Conducta  Cantidad_casos\n",
       "295     2009-01  Homicidio             104\n",
       "296     2009-02  Homicidio             110\n",
       "297     2009-03  Homicidio             113\n",
       "298     2009-04  Homicidio             186\n",
       "299     2009-05  Homicidio             209\n",
       "..          ...        ...             ...\n",
       "443     2021-05  Homicidio              37\n",
       "444     2021-06  Homicidio              44\n",
       "445     2021-07  Homicidio              28\n",
       "446     2021-08  Homicidio              30\n",
       "447     2021-09  Homicidio              30\n",
       "\n",
       "[153 rows x 3 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Conducta\"] == \"Homicidio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Cantidad_casos\"] = pd.to_numeric(df[\"Cantidad_casos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nmcbu\\anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator CountVectorizer from version 0.23.2 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\nmcbu\\anaconda3\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator MultinomialNB from version 0.23.2 when using version 0.22.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "sentiment = sentiment_analysis.SentimentAnalysisSpanish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.DataFrame({\"length\": range(1,1000)})\n",
    "for date in df['Fecha_hecho'].unique():\n",
    "    df4 = pd.read_csv(date + \"_clean.csv\", encoding = \"latin\")\n",
    "    sentiment_list = []\n",
    "    for tweet in df4[\"tweet\"]: \n",
    "        sentiment_list.append(sentiment.sentiment(tweet))\n",
    "    training_set[date] = pd.Series(sentiment_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = training_set.fillna(.5)\n",
    "training_set.drop(\"length\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = training_set.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    " Homicides = df[df[\"Conducta\"] == \"Homicidio\"][\"Cantidad_casos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>989</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2009-01</th>\n",
       "      <td>7.571231e-06</td>\n",
       "      <td>0.210686</td>\n",
       "      <td>1.078549e-01</td>\n",
       "      <td>0.838480</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.093317</td>\n",
       "      <td>0.449385</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.602592</td>\n",
       "      <td>4.872802e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-02</th>\n",
       "      <td>5.365300e-01</td>\n",
       "      <td>0.296440</td>\n",
       "      <td>2.717961e-02</td>\n",
       "      <td>0.002803</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>0.952857</td>\n",
       "      <td>0.006412</td>\n",
       "      <td>0.741496</td>\n",
       "      <td>0.297196</td>\n",
       "      <td>4.155509e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-03</th>\n",
       "      <td>2.120335e-01</td>\n",
       "      <td>0.497892</td>\n",
       "      <td>1.252861e-03</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.778197</td>\n",
       "      <td>0.487280</td>\n",
       "      <td>0.437548</td>\n",
       "      <td>0.012598</td>\n",
       "      <td>0.476638</td>\n",
       "      <td>4.108120e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-04</th>\n",
       "      <td>2.515014e-01</td>\n",
       "      <td>0.029786</td>\n",
       "      <td>6.934223e-02</td>\n",
       "      <td>0.630631</td>\n",
       "      <td>0.061294</td>\n",
       "      <td>0.487280</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.497892</td>\n",
       "      <td>0.073071</td>\n",
       "      <td>7.918687e-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2009-05</th>\n",
       "      <td>3.597197e-03</td>\n",
       "      <td>0.002655</td>\n",
       "      <td>7.928068e-01</td>\n",
       "      <td>0.046609</td>\n",
       "      <td>0.021849</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.314300</td>\n",
       "      <td>0.043096</td>\n",
       "      <td>0.002655</td>\n",
       "      <td>4.309571e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05</th>\n",
       "      <td>4.646954e-05</td>\n",
       "      <td>0.074425</td>\n",
       "      <td>6.874465e-04</td>\n",
       "      <td>0.149023</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.867639</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.010878</td>\n",
       "      <td>6.995389e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06</th>\n",
       "      <td>4.623355e-01</td>\n",
       "      <td>0.999960</td>\n",
       "      <td>1.009327e-01</td>\n",
       "      <td>0.059407</td>\n",
       "      <td>0.956348</td>\n",
       "      <td>0.999930</td>\n",
       "      <td>0.533377</td>\n",
       "      <td>0.024706</td>\n",
       "      <td>0.067701</td>\n",
       "      <td>6.487025e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07</th>\n",
       "      <td>5.116181e-01</td>\n",
       "      <td>0.372685</td>\n",
       "      <td>4.933628e-01</td>\n",
       "      <td>0.060660</td>\n",
       "      <td>0.173895</td>\n",
       "      <td>0.017999</td>\n",
       "      <td>0.108951</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>4.329576e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08</th>\n",
       "      <td>5.286088e-08</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>4.855032e-08</td>\n",
       "      <td>0.515944</td>\n",
       "      <td>0.991556</td>\n",
       "      <td>0.081692</td>\n",
       "      <td>0.342408</td>\n",
       "      <td>0.101418</td>\n",
       "      <td>0.128402</td>\n",
       "      <td>5.568924e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09</th>\n",
       "      <td>4.515859e-02</td>\n",
       "      <td>0.168152</td>\n",
       "      <td>9.455529e-03</td>\n",
       "      <td>0.458453</td>\n",
       "      <td>0.606032</td>\n",
       "      <td>0.339461</td>\n",
       "      <td>0.374758</td>\n",
       "      <td>0.446173</td>\n",
       "      <td>0.924227</td>\n",
       "      <td>1.265873e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows Ã— 999 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1             2         3         4         5    \\\n",
       "2009-01  7.571231e-06  0.210686  1.078549e-01  0.838480  0.001233  0.093317   \n",
       "2009-02  5.365300e-01  0.296440  2.717961e-02  0.002803  0.004004  0.952857   \n",
       "2009-03  2.120335e-01  0.497892  1.252861e-03  0.000197  0.778197  0.487280   \n",
       "2009-04  2.515014e-01  0.029786  6.934223e-02  0.630631  0.061294  0.487280   \n",
       "2009-05  3.597197e-03  0.002655  7.928068e-01  0.046609  0.021849  0.000121   \n",
       "...               ...       ...           ...       ...       ...       ...   \n",
       "2021-05  4.646954e-05  0.074425  6.874465e-04  0.149023  0.000468  0.867639   \n",
       "2021-06  4.623355e-01  0.999960  1.009327e-01  0.059407  0.956348  0.999930   \n",
       "2021-07  5.116181e-01  0.372685  4.933628e-01  0.060660  0.173895  0.017999   \n",
       "2021-08  5.286088e-08  0.000337  4.855032e-08  0.515944  0.991556  0.081692   \n",
       "2021-09  4.515859e-02  0.168152  9.455529e-03  0.458453  0.606032  0.339461   \n",
       "\n",
       "              6         7         8             9    ...  989  990  991  992  \\\n",
       "2009-01  0.449385  0.000015  0.602592  4.872802e-01  ...  0.5  0.5  0.5  0.5   \n",
       "2009-02  0.006412  0.741496  0.297196  4.155509e-01  ...  0.5  0.5  0.5  0.5   \n",
       "2009-03  0.437548  0.012598  0.476638  4.108120e-01  ...  0.5  0.5  0.5  0.5   \n",
       "2009-04  0.005267  0.497892  0.073071  7.918687e-04  ...  0.5  0.5  0.5  0.5   \n",
       "2009-05  0.314300  0.043096  0.002655  4.309571e-02  ...  0.5  0.5  0.5  0.5   \n",
       "...           ...       ...       ...           ...  ...  ...  ...  ...  ...   \n",
       "2021-05  0.374000  0.000118  0.010878  6.995389e-01  ...  0.5  0.5  0.5  0.5   \n",
       "2021-06  0.533377  0.024706  0.067701  6.487025e-07  ...  0.5  0.5  0.5  0.5   \n",
       "2021-07  0.108951  0.000011  0.000911  4.329576e-02  ...  0.5  0.5  0.5  0.5   \n",
       "2021-08  0.342408  0.101418  0.128402  5.568924e-01  ...  0.5  0.5  0.5  0.5   \n",
       "2021-09  0.374758  0.446173  0.924227  1.265873e-01  ...  0.5  0.5  0.5  0.5   \n",
       "\n",
       "         993  994  995  996  997  998  \n",
       "2009-01  0.5  0.5  0.5  0.5  0.5  0.5  \n",
       "2009-02  0.5  0.5  0.5  0.5  0.5  0.5  \n",
       "2009-03  0.5  0.5  0.5  0.5  0.5  0.5  \n",
       "2009-04  0.5  0.5  0.5  0.5  0.5  0.5  \n",
       "2009-05  0.5  0.5  0.5  0.5  0.5  0.5  \n",
       "...      ...  ...  ...  ...  ...  ...  \n",
       "2021-05  0.5  0.5  0.5  0.5  0.5  0.5  \n",
       "2021-06  0.5  0.5  0.5  0.5  0.5  0.5  \n",
       "2021-07  0.5  0.5  0.5  0.5  0.5  0.5  \n",
       "2021-08  0.5  0.5  0.5  0.5  0.5  0.5  \n",
       "2021-09  0.5  0.5  0.5  0.5  0.5  0.5  \n",
       "\n",
       "[153 rows x 999 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_set\n",
    "y = Homicides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107, 999)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 999)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=1000,activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(units=500,activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "model.compile(optimizer='adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1/1 [==============================] - 0s 382ms/step - loss: 10203.1846 - val_loss: 6834.1401\n",
      "Epoch 2/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10154.2822 - val_loss: 6833.3696\n",
      "Epoch 3/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3936 - val_loss: 6833.3174\n",
      "Epoch 4/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3320 - val_loss: 6833.3086\n",
      "Epoch 5/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3223 - val_loss: 6833.3057\n",
      "Epoch 6/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3203 - val_loss: 6833.3052\n",
      "Epoch 7/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 8/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 9/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 10/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 11/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 12/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 13/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 14/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 15/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 16/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 17/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 18/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 19/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 20/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 21/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 22/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 23/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 24/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 25/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 26/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 27/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 28/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 29/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 30/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 31/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 32/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 33/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 34/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 35/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 36/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 37/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 38/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 39/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 40/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 41/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 42/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 43/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 44/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 45/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 46/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 47/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 48/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 49/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 50/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 51/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 52/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 53/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 54/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 55/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 56/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 57/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 58/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 59/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 60/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 61/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 62/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 63/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 64/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 65/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 66/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 67/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 68/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 69/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 70/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 71/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 72/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 73/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 74/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 75/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 76/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 77/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 78/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 79/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 80/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 81/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 82/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 83/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 84/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 85/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 86/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 87/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 88/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 89/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 90/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 91/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 92/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 93/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 94/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 95/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 96/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 97/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 98/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 99/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 100/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 101/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 102/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 103/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 104/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 105/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 106/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 107/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 108/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 109/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 110/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 111/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 112/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 113/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 114/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 115/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 116/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 117/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 118/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 119/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 120/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 121/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 122/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 123/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 124/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 125/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 126/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 127/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 128/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 129/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 130/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 131/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 132/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 133/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 134/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 135/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 136/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 137/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 138/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 139/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 140/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 141/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 142/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 143/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 144/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 145/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 146/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 147/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 148/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 149/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 150/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 151/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 152/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 153/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 154/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 156/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 157/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 158/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 159/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 160/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 161/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 162/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 163/400\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 164/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 165/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 166/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 167/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 168/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 169/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 170/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 171/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 172/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 173/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 174/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 175/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 176/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 177/400\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 178/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 179/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 180/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 181/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 182/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 183/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 184/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 185/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 186/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 187/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 188/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 189/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 190/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 191/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 192/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 193/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 194/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 195/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 196/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 197/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 198/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 199/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 200/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 201/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 202/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 203/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 204/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 205/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 206/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 207/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 208/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 209/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 210/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 211/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 212/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 213/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 214/400\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 215/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 216/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 217/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 218/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 219/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 220/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 221/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 222/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 223/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 224/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 225/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 226/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 227/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 228/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 229/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 230/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 231/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 232/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 233/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 234/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 235/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 236/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 237/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 238/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 239/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 240/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 241/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 242/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 243/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 244/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 245/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 246/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 247/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 248/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 249/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 250/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 251/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 252/400\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 253/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 254/400\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 255/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 256/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 257/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 258/400\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 259/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 260/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 261/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 262/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 263/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 264/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 265/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 266/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 267/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 268/400\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 269/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 270/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 271/400\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 272/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 273/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 274/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 275/400\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 276/400\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 277/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 278/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 279/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 280/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 281/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 282/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 283/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 284/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 285/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 286/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 287/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 288/400\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 289/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 290/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 291/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 292/400\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 293/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 294/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 295/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 296/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 297/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 298/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 299/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 300/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 301/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 302/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 303/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 304/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 305/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 306/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 307/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 308/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 309/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 310/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 311/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 312/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 313/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 314/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 315/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 316/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 317/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 318/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 319/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 320/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 321/400\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 322/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 323/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 324/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 325/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 326/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 327/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 328/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 329/400\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 330/400\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 331/400\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 332/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 333/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 334/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 335/400\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 336/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 337/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 338/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 339/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 340/400\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 341/400\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 342/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 343/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 344/400\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 345/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 346/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 347/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 348/400\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 349/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 350/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 351/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 352/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 353/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 354/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 355/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 356/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 357/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 358/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 359/400\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 360/400\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 361/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 362/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 363/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 364/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 365/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 366/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 367/400\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 368/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 369/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 370/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 371/400\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 372/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 373/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 374/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 375/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 376/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 377/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 378/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 379/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 380/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 381/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 382/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 383/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 384/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 385/400\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 386/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 387/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 388/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 389/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 390/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 391/400\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 392/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 393/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 394/400\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 395/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 396/400\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 397/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 398/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 399/400\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10153.3174 - val_loss: 6833.3042\n",
      "Epoch 400/400\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10153.3174 - val_loss: 6833.3042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19f655b0dc8>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train,y=y_train.values,\n",
    "          validation_data=(X_test,y_test.values),\n",
    "          batch_size=128,epochs=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame(model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x19f66ac4588>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.52180888730547"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.66386315959474"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.121645841819287e-08"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_variance_score(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19f6161fb88>]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_test,predictions)\n",
    "\n",
    "# Perfect predictions\n",
    "plt.plot(y_test,y_test,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
